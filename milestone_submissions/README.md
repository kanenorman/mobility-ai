<div align="center">
  <img src="../assets/figures/mobility_ai_logo.png" alt="Mobility AI Logo" width="400"/>

<p style="color: #800080;"><em>Navigate the future, today</em></p>
</div>

## MBTArrivals-App: Realtime navigation for transit systems in Massachusetts

**Note: Realtime in this context refers to the predictions generated by our machine learning workflows; the models generating these predictions are trained offline to preserve computational resources**

# AC215 - Milestone2

**Table of Contents**

- [Introduction](#introduction)
- [Micro Service Descriptions](#micro-service-descriptions)
  - [Zookeeper](#zookeeper)
  - [Broker1, Broker2, Broker3](#brokers)
  - [Schema-Registry](#schema-registry)
  - [Control-Center](#control-center)
  - [Producer](#producer)
  - [Consumer](#consumer)
  - [Flask App](#flask-app)
  - [Flask Database](#flask-database)
- [Project Organization](#project-organization)
- [Running Locally](#running-locally)
- [Architecture Highlights](#architecture-highlights)
- [Machine Learning Workflow Implementation](#machine-learning-workflow-implementation)
- [Key Files and Their Functions](#key-files-and-their-functions)
- [Concluding Remarks](#concluding-remarks)

## Introduction

1. **Team Members:** Kane Norman, Juan Castillo, Philip Ndikum, David Wintermeyer
1. **Group Name:** MBTArrivals
1. **Project:** Amidst growing difficulties in urban mobility and the need for timely transit predictions, our group project aims to build a scalable prediction system for the Massachusetts Bay Transportation Authority (MBTA) transit system. By combining modern data engineering and scalable Machine Learning operations (MLOps) methodologies, this research aims to address pressing challenges in infrastructure and transit reliability -- with broader implications for supply-chain optimization and logistics. By providing guidelines and solutions, our project serves as an instructive blueprint, and a substantive contribution to the ever-growing data engineering and urban mobility literature.

**Team Members**
Kane Norman, Juan Castillo, Philip Ndikum, David Wintermeyer

**Group Name**
MBTArrivals

**Project**
Amidst growing difficulties in urban mobility and the need for timely transit predictions, our
group project aims to build a scalable forecasting system for the Massachusetts
Bay Transportation Authority (MBTA) transit system. By combining modern data engineering and scalable Machine Learning operations (MLOps) methodologies, this research aims to address pressing challenges in infrastructure and transit reliability -- with broader implications for supply-chain optimization and logistics. By providing guidelines and solutions, our project serves as an instructive blueprint, and a substantive contribution to the ever-growing data engineering and urban mobility literature.

### Micro Service Descriptions

In this project, we implement a microservice architecture where each service is containerized using Docker
and orchestrated via a `docker-compose.yml` file. Here's an overview of the services:

#### Zookeeper

Zookeeper is a crucial distributed coordination service that plays a vital role in managing distributed systems.
In this project, it is primarily utilized within the Apache Kafka ecosystem for tasks such as maintaining configuration information,
ensuring distributed synchronization, and providing a naming service.

#### Broker1, Broker2, Broker3

Brokers serve as the backbone of a Kafka infrastructure. They are responsible for receiving messages from producers,
securely storing these messages, and efficiently delivering them to consumers. We employ multiple broker instances within our Kafka cluster for redundancy
to ensure fault tolerance and scalability -- enhancing the robustness of our system.

#### Schema-Registry

The Schema Registry service is integral to our data pipeline, enabling the storage and management of Avro schemas
associated with Kafka topics. Its primary function is to ensure data consistency and compatibility between producers and consumers --
facilitating the seamless exchange of information within our architecture.

#### Control-Center

The Control Center is an indispensable tool for managing and monitoring Apache Kafka clusters.
It provides valuable insights into the health and performance of our Kafka infrastructure, helping us maintain a
resilient and responsive messaging system.

#### Producer

The Producer service is responsible for generating and dispatching messages to Kafka topics.
Its primary data source is the [mbta-v3-api](https://www.mbta.com/developers/v3-api), which supports real-time data streaming
through Server Sent Events (SSE). This service is pivotal in ensuring that fresh data is continuously ingested into our system.
The bulk of this code can be found in the `/data_streaming/kafka_producer` directory.

#### Consumer

The Consumer service plays a vital role in processing messages from Kafka topics. It leverages the power of
[Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
to efficiently extract and process data from Kafka. This enables us to perform complex data transformations and analytics,
driving our data-driven decision-making processes.
The bulk of this code can be found in the `/data_streaming/kafka_consumer` directory.

#### Flask App

Our Flask App is the user-facing component of our system, built using the Flask web framework.
We employ Flask's Jinja2 templating framework for the frontend, while Bluma CSS handles our CSS styling.
This combination ensures an interactive and visually appealing web interface for our users, enhancing their overall experience.
The bulk of this code can be found in the `/flask_app/app` directory.

#### Flask Database

The Flask Database service is a PostgreSQL database at the core of our data storage and retrieval system.
To facilitate spatial data processing, we have integrated the PostGIS extension into our database.
This extension enables us to handle spatial data effectively, a critical requirement for our predictive models and geospatial applications.
The bulk of this code can be found in the `/postgres` and `/flask_app/app/views` directories.

In the future, once our model is developed, we will make an API endpoint for the model and run it as a separate microservice.

## Project Organization

```
├── LICENSE
├── README.md
├── SETUP_INSTRUCTIONS.md
├── assets
│   └── figures
│       ├── data-streaming.svg
│       ├── high-level.svg
│       ├── mobility_ai_logo.png
│       ├── wanddb_monitoring.pdf
│       └── wanddb_monitoring.png
├── data_streaming
│   ├── README.md
│   ├── kafka_consumer
│   │   ├── Dockerfile
│   │   ├── __init__.py
│   │   ├── consumer.py
│   │   ├── schemas
│   │   │   ├── __init__.py
│   │   │   ├── alerts.py
│   │   │   ├── schedules.py
│   │   │   ├── shapes.py
│   │   │   ├── stops.py
│   │   │   ├── trips.py
│   │   │   └── vehicles.py
│   │   └── utils
│   │       ├── __init__.py
│   │       ├── config.py
│   │       └── spark_functions.py
│   └── kafka_producer
│       ├── Dockerfile
│       ├── __init__.py
│       ├── config.py
│       └── producer.py
├── dev-requirements.txt
├── docker-compose.yml
├── flask_app
│   ├── Dockerfile
│   └── app
│       ├── __init__.py
│       ├── config
│       │   ├── __init__.py
│       │   └── config.py
│       ├── extensions.py
│       ├── models
│       │   ├── __init__.py
│       │   ├── alert.py
│       │   ├── schedule.py
│       │   ├── shape.py
│       │   ├── stop.py
│       │   └── trip.py
│       ├── static
│       │   ├── images
│       │   │   └── train-icon.png
│       │   └── js
│       │       ├── index.js
│       │       └── stops.js
│       ├── templates
│       │   └── index.html
│       └── views.py
├── machine_learning
│   ├── delay_etl.py
│   ├── gcp_dataloader.py
│   ├── milestone_3_readme.md
│   └── ml_train.py
├── milestone_submissions
│   ├── README.md
│   └── milestone2
├── postgres
│   ├── Dockerfile
│   └── db
│       └── init.sql
├── pytest.ini
├── requirements.txt
└── tests
    ├── __init__.py
    ├── kafka_consumer_utils_spark_functions_test.py
    └── kafka_producer_configs_test.py
```

### Running Locally

1. **Clone the Repository**:

   ```bash
   git clone https://github.com/kanenorman/AC215_MBTArrivals-App.git
   cd AC215_MBTArrivals-App
   ```

1. **Request an API Token**:

   - Visit the [MBTA's official site](https://www.mbta.com/developers/v3-api) or the relevant link to get your API token.

1. **Set Up Your Environment**:

   - Create a local `.env` file in the project directory.
   - Populate the `.env` file with necessary configurations, including your MBTA API Token.

   ** Example: `.env` file **
   This section lists the configuration variables used in the project along with corresponding values.

   ```bash
    # kafka & zookeeper
    KAFKA_HOST1=broker1
    KAFKA_PORT1=9092
    KAFKA_HOST2=broker2
    KAFKA_PORT2=19092
    KAFKA_HOST3=broker3
    KAFKA_PORT3=29092
    ZOOKEEPER_PORT=2181

    # API keys
    MBTA_API_KEY= [Your_key]

    # Postgres
    POSTGRES_PORT=5432
    POSTGRES_DB=mbta
    POSTGRES_USER=postgres
    POSTGRES_PASSWORD=password
    POSTGRES_HOST=flask_database
    POSTGRES_DRIVER=org.postgresql.Driver

    # Flask
    FLASK_PORT=5000

    #W&B
    WANDB_API_KEY=[Your_key]
   ```

1. **Set up Python Version using Pyenv**:

   - If you haven't installed `pyenv` yet, you can do so by following the instructions on [pyenv's GitHub repository](https://github.com/pyenv/pyenv#installation).
   - Install the required Python version:
     ```bash
     pyenv install 3.10.0
     pyenv local 3.10.0
     ```
   - Verify the activated Python version:
     ```bash
     python --version
     ```

1. **Set up and Activate Conda Environment**:

   - Create and activate a new Conda environment named "mbta_env" with Python 3.10 and install requirements:
     ```bash
     conda config --add channels conda-forge # Ensure extra channels added
     conda create --name mbta_env python=3.10
     conda activate mbta_env
     pip install -r requirements.txt
     ```

1. **Ensure Docker is Running (For Docker Users)**:

   ```bash
   sudo systemctl start docker
   sudo systemctl status docker
   ```

1. **Run the App with Docker**:

   ```bash
   docker-compose up -d
   ```

1. **Access the App**:
   - Open a web browser and navigate to `localhost:5000`.

## Milestone 3: Building a Scalable and Modular Computing Infrastructure

1. **MS3 overview:** This milestone focuses on constructing a robust, scalable, and modular computing infrastructure. With an emphasis on adaptability, we've architected a system capable of seamless integration with a plethora of tools and cloud services.

## Architecture Highlights

1. **Distributed Computing and Cloud Storage Integration**:
   - Google Cloud Platform (GCP) Our primary cloud storage solution. By leveraging GCP, we can use a plethora of tools and services, including Google Colab Pro for expansive experimentation. Whilst we selected GCP based on the deliverable objectives, we tried to use platform agnostic tools and version controlled our code on Github to ensure our architecture isn't confined to one ecosystem or vendor, reflecting a broad, real-world approach where flexibility and scalability are paramount.
   - Data Pipeline: The design embodies robust extraction, transformation, and versioning capabilities. Examples of versioned datasets underline its effectiveness.
1. **Data Management**: While our current phase primarily harnesses tools outside the TensorFlow ecosystem, our architectural decisions leave the door open for seamless integration with TensorFlow's utilities (like TF Data and TF Records) in future iterations. With regard to the current milestone, we decided against leveraging TF Data and Records since we are working with a small subset of data that can be run locally. As we expand our dataset, we will implement TF Data and Records if the dataset grows in size to the point where it would be beneficial. By design, we have a robust and flexible architecture allowing us to use different libraries and tools to scale out our end-to-end architecture.

## Machine Learning Workflow Implementation

Our advanced training workflows are meticulously designed to offer a robust yet flexible approach, ensuring both immediate results and long-term scalability. With evidence of successful training runs, in-depth experiment tracking, and provisions for multi-GPU/serverless training, this system is positioned to evolve with changing demands and technologies.

Furthermore, the production-grade design of our architecture affords flexibility. It's equipped to seamlessly transition between different server-less vendors, ensuring we're not tethered to a specific service. This scalability is vital as it allows for easy incorporation of enhanced data sources. Over the coming weeks, our exploration will extend to potentially integrating new data streams, such as web APIs, which could further enrich our analyses and forecasts.

### Key Files and Their Functions

1. **gcp_dataloader.py**

   - **Purpose**: Enables seamless extraction and preprocessing of transit data directly from Google Cloud Platform buckets.
   - **Functions**:
     - `authenticate_gcp()`: Facilitates GCP authentication.
     - `extract_from_gcp()`: Initiates raw data retrieval via SQL operations.
     - `preprocess_data()`: Implements data preprocessing routines.

2. **delay_etl.py**

   - **Purpose**: Primarily engaged in transit data preprocessing, tailored for predicting bus delays.
   - **Functions**:
     - `create_date_features()`: Specialized feature engineering for date-time data.
     - `transform()`: ETL functions customized for delay-centric data.
     - `data_checks_and_cleaning()`: Ensures data integrity through sanitization and cleaning.

3. **ml_train.py**
   - **Purpose**: A central hub for GPU-optimized training, exhaustive model optimization, performance assessment, and model serialization.
   - **Functions**:
     - `compute_metrics_table()`: Produces a comprehensive metrics table suited for regression analysis.
     - `retrain_best_xgboost()`: Refines models post-tuning using the combined prowess of Ray and WandB.
     - `retrain_model_with_best_config()`
     - `train_mbta()`: Core functions dedicated to effective model training.

## Concluding Remarks

Our approach meets all requirements laid out in the Milestone 3 framework. We've forged a resilient and flexible architecture, ensuring we're poised to scale and adapt in forthcoming phases. As we look ahead, the upcoming milestones will see us delving into financial estimations concerning data quality control for the government and integrating additional data to increase the robustness and accuracy of our predictions. We are optimistic that our findings has the potential to foster tangible, positive impacts for commuters, extending beyond the confines of this project.

One important feature of our workflow is monitoring and experiment tracking through the Weights and Biases (wandb) cloud platform. Below is a snapshot of the monitoring interface:

![WandB Monitoring](../assets/figures/wanddb_monitoring.png)

This platform provides real-time feedback on our training sessions, assisting in keeping our models optimized and efficient -- which will facilitate scaling out our prediction system.


# Milestone 4


In this milestone, we containerized our machine learning application within Docker and deployed containers using Google’s Artifact Registry (GAR). The structure of our application adheres to best modern practices for machine learning projects -- promoting modularity, scalability, and maintainability.

## Table of Contents

- [Milestone 4](#milestone-4)
  - [Breaking Down the Directory Structure](#breaking-down-the-directory-structure)
- [MLOps: Adopting a Serverless Microservice Architecture](#mlops-adopting-a-serverless-microservice-architecture)
- [Model Evolution & Continuous Improvement](#model-evolution--continuous-improvement)
- [Model Selection & Design: Aligning with the Markscheme and Real-world Constraints](#model-selection--design-aligning-with-the-markscheme-and-real-world-constraints)
  - [Markscheme: Distillation, Quantization, and Compression](#markscheme-distillation-quantization-and-compression)
  - [Markscheme: Vertex AI Pipelines (Kubeflow) Deployment Approach](#markscheme-vertex-ai-pipelines-kubeflow-deployment-approach)
    - [Setting Permissions](#setting-permissions)
    - [Local Testing](#local-testing)
    - [Deployment to Google Artifact Registry (GAR) and Vertex AI](#deployment-to-google-artifact-registry-gar-and-vertex-ai)
    - [Pushing Docker Image to GAR and Vertex AI](#pushing-docker-image-to-gar-and-vertex-ai)

```
➜  machine_learning_app
.
├── authenticate.py
├── config.py
├── data
│   ├── ml_transit_training_data.csv
│   └── raw_transit_data.csv
├── dl
│   ├── dnn_pruner.py
│   ├── dnn_trainer.py
│   ├── __init__.py
│   └── knowledge_distiller.py
├── etl
│   ├── gcp_dataloader.py
│   ├── __init__.py
│   └── xgboost_etl.py
├── experiments
├── __init__.py
├── ml
│   ├── __init__.py
│   ├── ml_utils.py
│   └── xgboost_trainer.py
├── models
├── production_models
    ├── final_best_xgboost.json
    └── .gitkeep
```

### Breaking Down the Directory Structure:
The directory structure below was designed with an emphasis on clarity, modularity, and task specificity in mind. This represents our commitment to following industry best practices by streamlining development and ensuring the robustness and scalability of our application.

- **Dockerfile.training**: This Dockerfile encapsulates the environment required for model training, catering to the computational and library dependencies necessary for this intensive task. By segmenting our Dockerfiles based on their function, we enhance container efficiency and maintainability. In Milestone 6, we plan to introduce "Dockerfile.testing", tailored for inference operations. This will ensure a lightweight and optimized container for real-time predictions, In line with our principle of modularity which is a hallmark of production-grade deployment practices.

- **authenticate.py**: A centralized module for authentication. By isolating authentication, the application guarantees a standardized, secure mechanism for verifying identity -- crucial for maintaining data integrity and control across microservice modules.

- **config.py**: This file serves as the hub for global configurations, establishing a unified interface for parameters. This promotes coherence across modules, reducing the risk of discrepancies arising from local configurations.

- **data**: This directory, reserved for datasets, ensures a universal data store. By categorically separating raw from processed datasets, the application mitigates confusion, streamlines data retrieval ,and ensures operations are performed on the correct data version.

- **dl**: Tailored for deep learning operations:
    - **dnn_pruner.py**: Emphasizes model efficiency and generalizabality by pruning neural network weights and neurons --  crucial for deployment in resource-constrained environments.
    - **dnn_trainer.py**: Trains a specified deep neural networks architecture, promoting optimal model convergence and generalization.
    - **knowledge_distiller.py**: Conducts model compression by leveraging knowledge distillation, critical for deploying large models on edge devices.

- **etl**: Standing for Extract, Transform, Load, this directory centralizes data processing workflows. Specifically, it manages these processes:
    - **gcp_dataloader.py**: Promotes iterability by providing specific utilities to fetch data from the Google Cloud Platform, and leverage cloud resources efficiently.
    - **xgboost_etl.py**: Tailors ETL processes for XGBoost, satisfying model-specific data preparation.

- **experiments**: Organized chronologically and serves as a log of model experiments. By structuring it this way, it becomes simpler to track model evolution, and leveraging tools like `ray` & `wandb` to optimize experiment tracking and comparison.

- **ml**: This section is dedicated to classical machine learning tasks:
    - **ml_utils.py**: Functions as a utility belt, offering an assortment of tools and functions, adhering to the need for reusable components in ML workflows.
    - **xgboost_trainer.py**: Dedicated to the XGBoost training lifecycle, reflecting the importance of modularizing training tasks based on the algorithm. This conducts robust hyperparameter tuning and model saving.

- **models**: Acts as a repository for transitional model states. By cataloging these intermediate artifacts, the application provides a granular history of model evolution, allowing for necessary rollbacks and comparison between iterations. These will be models which can be pushed to the Google Artifact Registry (GAR) for inference in Milestone 6.

- **production_models**: Distinctly houses deployment-ready models. By isolating production-grade models, the structure ensures that only fully validated, optimal models are promoted to production environments, minimizing potential deployment risks.

- **README.md**: More than just a file, it embodies the principle of thorough documentation, essential for team collaboration, onboarding new members, and offering external entities an overview of the application's design and functionalities.


## MLOps: Adopting a Serverless Microservice Architecture

In the evolving landscape of MLOps, we've strategically designed our machine learning application around a serverless microservice architecture. This design paradigm not only aligns with contemporary application deployment practices but also offers numerous operational and management benefits. Here's a deeper dive into our architectural choices and their implications:

Serverless computing isn't just a trendy concept; it's a transformative architectural choice, focusing our attention on the application's core functionality, sidestepping concerns of infrastructure management and server provisioning typically associated with traditional architectures.

- **Seamless Deployment and Scaling**: Serverless computing ensures our application effortlessly scales with demand. As traffic grows, the infrastructure scales up, and during lulls, it scales down — all without manual oversight.
- **Cost-Efficiency**: Billing in serverless architectures revolves around compute time. This pay-as-you-use model can translate to significant cost savings, particularly for applications experiencing varied traffic.
- **Production-Grade Training**: Our architecture distinctly separates ETL processes, traditional ML, and deep learning. This guarantees that updates to training algorithms don't inadvertently disrupt data preparation stages, preserving both data integrity and model reliability.
- **Optimized Inference**: A clear division between intermediate `models` and deployment-ready `production_models` ensures that only vetted, performance-optimized models are used in inference scenarios.
- **Independent Scalability**: The architecture supports the individual scaling of services. For example, a surge in computational needs for `xgboost_trainer.py` won't disrupt other services.
- **Enhanced Code Management**: Our modular approach allows for the independent development, testing, and deployment of services. This contained structure means issues in one module don't cascade to others, streamlining debugging and upkeep.
- **Tech Stack Flexibility**: The microservice approach permits disparate services to be built with varied tech stacks, aligning each service with its most suitable tools and frameworks.

## Model Evolution & Continuous Improvement

While our current system leverages the XGBoost model, our architecture's agility ensures it remains adaptable, ready to accommodate Bayesian, Machine, or Deep Learning models as requirements evolve. This forward-compatible design embodies a harmony between innovation and stability, perfectly setting the stage for our imminent deployments in MS5 and MS6.

Integral to our design is the emphasis on continuous integration and delivery (CI/CD). We've embedded CI/CD practices directly within Github, adhering to industry best practices, which are poised for future enhancements. In tandem, we've retained the model tracking utilities introduced in prior milestones:

- **WandB (Weights and Biases)**: An instrumental tool for performance visualization. WandB facilitates metric visualization, experiment comparison, and collaborative insights sharing.
- **Ray**: A powerful utility for hyperparameter optimization and experiment versioning, allowing us to harness distributed training across diverse compute resources efficiently.

## Model Selection & Design: Aligning with the Markscheme and Real-world Constraints

While we primarily utilized `xgboost_trainer.py` for model training, our system's design and codebase adhere to industry-standard best practices, showcasing the flexibility inherent in our architecture. This strategic decision was motivated by the dual objectives of retaining simplicity, to ensure lightweight Docker containers for both training and production stages, and catering to the markscheme's requirements.

Deep Learning (DL) models, especially for tasks like time-series forecasting, typically necessitate vast amounts of data to achieve production-grade optimization. Given the trade-offs associated with such intricate DL models, the more streamlined and computationally efficient XGBoost model was a judicious choice for our primary deployment. Nonetheless, to satisfy the comprehensive aspects of the markscheme, we ensured that our DL code was structured in an analogous manner, using `dl_trainer.py` as the counterpart to `xgboost_trainer.py`. Such consistent and clear naming conventions and structure attest to our commitment to adhering to proven Machine Learning Operations (MLOps) design patterns.

### Markscheme: Distillation, Quantization, and Compression

In the realm of model optimization and compression, we've incorporated a suite of techniques that reflect industry advancements:

- **Distillation/Quantization/Compression**:
    - **Knowledge Distillation**: This is a method where a simpler, smaller model (referred to as the 'student') is trained to replicate the behavior of a more complex, larger model (the 'teacher'). Our implementation facilitates an effective transfer of knowledge, ensuring the student model achieves comparable performance while being more efficient.
    - **Pruning**: We've employed both weight and neuron pruning. These techniques selectively remove model weights or neurons that contribute least to the final predictions, optimizing the model for faster inferencing and reduced memory footprint, especially vital for deployment in resource-constrained environments.

- **Key Python Modules for the Markscheme**:
    -  `dnn_pruner.py`:
        - Conducts weight and neuron pruning operations on DL models.
        - Executes fine-tuning on pruned models to recover any lost accuracy.
        - Assesses pruned models, selecting the most optimized version based on performance and compression metrics.

    - `dnn_trainer.py`:
        - Orchestrates training of deep neural networks on relevant datasets.
        - Manages hyperparameter optimization to ensure optimal performance for the DL model.

    - `knowledge_distiller.py`:
        - Employs an Object-Oriented Programming (OOP) approach to instantiate student models.
        - Oversees the distillation process, transferring knowledge from the teacher model to the student.
        - Spearheads hyperparameter tuning for the distilled student model.

In essence, while we've made informed trade-offs for the project's primary deployment, we have meticulously ensured alignment with the markscheme's stipulations, embodying a blend of academic rigor and real-world pragmatism.

## Markscheme: Vertex AI Pipelines (Kubeflow) Deployment Approach

In the realm of machine learning deployment, transitioning from preliminary stages to production-ready solutions can be intricate. However, by grounding our methodology on industry best practices right from inception, we drastically diminish potential deployment challenges. This emphasizes the vital role of a consistent and organized methodology throughout the machine learning lifecycle:

1. Our deployment to Vertex AI does not rely on Cloud Functions. Instead, we embraced a Dockerfile and command-line driven approach, utilizing the gCloud CLI tool. This methodological decision is grounded in its inherent flexibility, which renders our deployment infrastructure-agnostic. By this, we mean our deployment strategy is not tethered to Google Cloud but can easily adapt and transition across diverse cloud vendors such as AWS, Microsoft Azure, and more.
2. Such an approach is particularly significant in real-world scenarios where vendor lock-in is a concern. Businesses often prioritize flexibility to seamlessly migrate between cloud providers, preventing undue reliance on a single vendor and fostering competition. Therefore, while our approach aligns with the Markscheme, it further extends to capture real-world best practices, ensuring that our design decisions resonate with industry standards.
3.  It's crucial to underscore that while we have diligently adhered to the Markscheme's guidelines, our choices are also informed by the broader context of industry preferences, ensuring that our solutions not only meet academic standards but are also aligned with real-world industry constraints and expectations.

To provide evidence of our successful deployment to Vertex AI, please refer to the screenshot below:

<div align="center">
  <img src="./vertex_ai.jpg" alt="Screenshot of successful deployment to Vertex AI" width="900"/>
</div>

### Setting Permissions:
Before we delve into the deployment commands, it's paramount to ensure the appropriate permissions are granted.

1. Navigate to the GCP console.
2. Proceed to `IAM & Admin`.
3. Identify and select the member (which could be your user account or a service account) executing the commands.
4. Edit the member details and assign the role `Artifact Registry Writer` or confirm it possesses the `artifactregistry.repositories.uploadArtifacts` permission.

### Local Testing:
It's always a best practice to test your solution locally before deploying it to a remote server.

```bash
# Execute the xgboost_trainer locally to ensure ETL retrieves the correct data:
python -m mbta_ml.ml.xgboost_trainer
```

### Deployment to Google Artifact Registry (GAR) and Vertex AI:
```bash
# Construct the Docker container:
sudo docker build -f Dockerfile.training -t gcr.io/ac215-transit-prediction/mbta_ml:latest

# (OPTIONAL) If encountering issues, run interactively to diagnose:
docker run -it --rm --entrypoint /bin/bash gcr.io/ac215-transit-prediction/mbta_ml:latest

#  (OPTIONAL) Within the container, initiate the trainer script for debugging:
python ml.xgboost_trainer.py
```
### Pushing Docker Image to GAR and Vertex AI:
For a seamless deployment to the cloud, adhere to the following considerations:
1. Refrain from utilizing `sudo` with Docker during Google Cloud deployment. This could circumvent user-specific configurations and vital permissions imperative for authentication.
2. Disable VPN when deploying or interfacing with cloud services to avert potential network disruptions or obstructed connections.

```bash
# Configure Docker authentication for Google Artifact Registry (GAR):
gcloud auth configure-docker us-east1-docker.pkg.dev

# Label your Docker image for GAR:
docker tag gcr.io/ac215-transit-prediction/mbta_ml:latest us-east1-docker.pkg.dev/ac215-transit-prediction/mbta-ml-train/mbta-ml-train:latest

# Upload Docker image to GAR. Avoid `sudo` as it might interfere with configuration credentials:
docker push us-east1-docker.pkg.dev/ac215-transit-prediction/mbta-ml-train/mbta-ml-train:latest

# Specify the project and region:
gcloud config set project ac215-transit-prediction
gcloud config set ai/region us-east1

# Submit a Training Job to Vertex AI using gcloud CLI:
gcloud beta ai custom-jobs create \
  --display-name="MBTA ML Training Job" \
  --worker-pool-spec=machine-type="n1-standard-4",replica-count=1,container-image-uri="us-east1-docker.pkg.dev/ac215-transit-prediction/mbta-ml-train/mbta-ml-train:latest"
```


# Milestone 5

## Project Organization

```
.
├── assets
│   ├── architecture
│   │   ├── kafka-producer.svg
│   │   └── kafka-stream.svg
│   └── figures
│       ├── data-streaming.svg
│       ├── high-level.svg
│       ├── mobility_ai_logo.png
│       ├── wanddb_monitoring.pdf
│       └── wanddb_monitoring.png
├── data_streaming
│   ├── kafka_producer
│   │   ├── config.py
│   │   ├── Dockerfile
│   │   ├── __init__.py
│   │   ├── producer.py
│   │   └── README.md
│   ├── kafka_streams
│   │   ├── kafka-connect-jdbc
│   │   │   └── build_connectors.sh
│   │   ├── ksqldb-server
│   │   │   └── queries.sql
│   │   └── README.md
│   └── README.md
├── dev-requirements.txt
├── docker-compose.yml
├── flask_app
│   ├── app
│   │   ├── config
│   │   │   ├── config.py
│   │   │   └── __init__.py
│   │   ├── extensions.py
│   │   ├── __init__.py
│   │   ├── models
│   │   │   ├── __init__.py
│   │   │   ├── location.py
│   │   │   ├── route.py
│   │   │   ├── scheduled_arrival.py
│   │   │   ├── schedule.py
│   │   │   ├── stop.py
│   │   │   ├── trip.py
│   │   │   └── vehicle.py
│   │   ├── static
│   │   │   ├── about.txt
│   │   │   ├── android-chrome-192x192.png
│   │   │   ├── android-chrome-512x512.png
│   │   │   ├── apple-touch-icon.png
│   │   │   ├── css
│   │   │   │   └── styles.css
│   │   │   ├── favicon-16x16.png
│   │   │   ├── favicon-32x32.png
│   │   │   ├── favicon.ico
│   │   │   ├── images
│   │   │   │   ├── mobility_ai_logo.png
│   │   │   │   └── train-icon.png
│   │   │   ├── js
│   │   │   │   ├── index.js
│   │   │   │   └── stops.js
│   │   │   └── site.webmanifest
│   │   ├── templates
│   │   │   ├── about.html
│   │   │   ├── base.html
│   │   │   ├── copyright.html
│   │   │   ├── index.html
│   │   │   ├── navigation.html
│   │   │   └── terms.html
│   │   └── views.py
│   └── Dockerfile
├── LICENSE
├── machine_learning_app
│   ├── assets
│   │   └── vertex_ai.jpg
│   ├── Dockerfile.training
│   ├── mbta_ml
│   │   ├── authenticate.py
│   │   ├── config.py
│   │   ├── data
│   │   │   ├── ml_transit_training_data.csv
│   │   │   └── raw_transit_data.csv
│   │   ├── etl
│   │   │   ├── gcp_dataloader.py
│   │   │   └── __init__.py
│   │   ├── experiments
│   │   │   ├── 25_10_2023
│   │   │   ├── 26_10_2023
│   │   │   └── 27_10_2023
│   │   ├── ml
│   │   │   ├── __init__.py
│   │   │   ├── ml_utils.py
│   │   │   └── xgboost_trainer.py
│   │   ├── models
│   │   ├── production_models
│   │   │   └── final_best_xgboost.json
│   │   └── README.md
│   ├── poetry.lock
│   ├── pyproject.toml
│   ├── README.md
│   └── requirements.txt
├── milestone_submissions
│   ├── README.md
│   └── vertex_ai.jpg
├── postgres
│   ├── db
│   │   └── init.sql
│   └── Dockerfile
├── pytest.ini
├── README.md
├── requirements.txt
├── SETUP_INSTRUCTIONS.md
└── tests
    ├── __init__.py
    └── kafka_producer_configs_test.py

```

## App Design, Setup, and Code Organization

In designing the architecture of our application, we have meticulously embraced a microservices-based approach, a decision rooted in industry best practices for scalable, robust software engineering. This approach is not only about achieving modularity but also about ensuring future scalability, vital for real-world applications. Our code organization is structured with an acute emphasis on maintainability and clarity, aligning with the highest standards of software engineering and the milestone markschemes.

![Screencase of MS5 video app](../assets/ms5_mbta_screen_record.gif)

- **Kafka Producer**: Implementing a Kafka Producer to initiate data ingestion represents a strategic choice for efficient data stream management. This component efficiently channels data streams to Kafka Message Brokers, exemplifying a modern approach to handling large volumes of data in real-time applications.
- **Kafka Message Broker**: The Kafka Message Broker is the backbone of our architecture. It adeptly manages data flow between components, ensuring both robustness and scalability. This choice underpins our commitment to building a system that can handle high throughput and low latency, essential for large-scale, real-time data processing.
- **Data Processing to PostgreSQL**: Our utilization of PostgreSQL for data processing and storage aligns with industry norms for reliable and effective data management. This setup ensures high performance, particularly in handling complex queries and vast datasets.
- **FlaskSQLAlchemy Integration**: The integration of FlaskSQLAlchemy is a testament to our dedication to seamless interaction between our Flask applications and the PostgreSQL database. This ORM (Object-Relational Mapping) tool simplifies database transactions, enhancing the efficiency and reliability of our data operations.
- **Flask with MapBox and Google Client**: Enriching our Flask application with dynamic data visualization tools such as MapBox and Google Client showcases our focus on creating an interactive and user-friendly frontend. These tools elevate the user experience, providing intuitive and informative data visualizations.
- **FastAPI for Model Serving**: Employing FastAPI for model serving aligns with contemporary practices for API development. This framework is known for its high performance and ease of use, making it an ideal choice for exposing our XGBoost model through a RESTful API, thereby ensuring streamlined communication across services.
- **XGBoost Model on Google Vertex AI**: Hosting our XGBoost model on Google Vertex AI aligns with the markscheme and demonstrates the utilization of modern cloud tools. This setup benefits from Kubernetes-powered autoscaling, ensuring that our machine learning model is both efficient and scalable, ready to handle varying loads with ease. It is important to note that our architecture is flexible to prevent vendor lock-in and increasing cloud costs, as we could host our applications with an "on-premise" infrastructure avoiding expensive Google costs. To adhere to the markscheme for milestone 6 we will integrate using Vertex AI but it is important to note we have factored in real-world industrial considerations.
- **API Development**: Our focus on developing robust APIs using FastAPI and Flask is a reflection of our commitment to effective backend-frontend communication. This is crucial for creating a seamless and responsive user experience, vital for modern web applications.
- **Frontend Design**: The intuitive and responsive design of our frontend, integrated seamlessly with backend APIs, ensures real-time data interactions. This approach to frontend design reflects a deep understanding of user needs and industry trends, prioritizing accessibility and responsiveness.
- **User Interface**: The integration of MapBox and Google Client within the user interface offers an interactive and informative user experience. This aspect of our project not only enhances user engagement but also serves as a model for effective data presentation in web applications.

![High Level Architecture](../assets/figures/high-level.svg)

Our architecture's flexibility and scalability make it suitable not just for the MBTA in Boston but also for potential expansion across America. The robustness of our design ensures that our state-of-the-art architecture can be scaled and adapted to various scenarios, making it an exemplary model in the realm of real-time data processing and visualization applications.

## Deployment Strategy and CI/CD

As an alternative to using Ansible as suggested by the markscheme, we plan on using GitHub Actions for our Continuous Integration and Continuous Deployment (CI/CD) pipeline, complemented by deployment on Google Cloud Platform (GCP) (MS6 Requirement). This strategic decision was driven by the desire to align with industry best practices, which increasingly favor the use of integrated CI/CD tools and cloud-native solutions for deployment. GitHub Actions provide a powerful, flexible, and developer-friendly environment for automating software workflows, directly within our code repository ecosystem. Coupled with GCP's scalable and reliable infrastructure, this approach not only meets but exceeds the robustness and efficiency requirements of modern application deployment strategies.

- **GitHub Actions & GitHub for CI/CD**: Leveraging these tools allowed us to automate our software development processes, from testing to deployment, within the GitHub ecosystem.
- **GCP Deployment**: Utilizing the robust and scalable infrastructure of Google Cloud Platform enabled us to achieve a high level of efficiency and reliability in our application deployment.
- **Streamlined Deployment Process**: Our CI/CD pipeline, built around GitHub Actions and GCP, ensures consistent and efficient deployment, enabling swift rollouts of updates and improvements, while maintaining high standards of security and compliance.

Our `docker-compose.yml` contains all the instructions required to deploy our application:

- **Zookeeper Service**: Uses the `confluentinc/cp-zookeeper:7.5.0` image, with the specified port and environment variables. It includes a health check using nc (netcat).
- **Kafka Brokers (broker1, broker2, broker3)**: Each broker uses the `confluentinc/cp-kafka:7.5.0` image. They are configured with individual broker IDs, Zookeeper connection settings, listener settings, and health checks. They depend on the Zookeeper service being healthy.
- **Schema Registry**: Utilizes the `confluentinc/cp-schema-registry:7.5.0` image and depends on the Kafka brokers and Kafka Connect service (`connect`) being healthy. It exposes port 8081 and is configured with environment variables for Kafka and schema registry settings.
- **Kafka Connect**: Based on `confluentinc/cp-kafka-connect:7.5.0`, this service depends on the Kafka brokers and a `flask_database` service. It exposes port 8083 and includes various Kafka Connect configurations, a volume mount for Kafka Connect JDBC, and a health check.
- **Sink Connectors**: Uses the `curlimages/curl:8.4.0` image, depending on `flask_database` and `connect`. It runs a script for building connectors.
- **Control Center**: Utilizes `confluentinc/cp-enterprise-control-center:7.5.0`, depending on Kafka brokers and `ksqldb-server`. It exposes port 9021 and is configured with various environment variables related to Kafka, Connect, KSQL, and schema registry.
- **Producer**: A custom service built from a Dockerfile in `./data_streaming/kafka_producer/Dockerfile`. It depends on Kafka brokers and includes environment variables for Kafka and an API key.
- **KSQLDB Server and CLI**: Both use Confluent's ksqlDB images (`confluentinc/ksqldb-server:0.29.0` and `confluentinc/ksqldb-cli:0.29.0`). The server exposes port 8088 and is configured with various KSQL settings, including a volume for SQL queries. The CLI depends on the server and Kafka brokers.
- **Flask App**: A custom service built from a Dockerfile in `./flask_app/Dockerfile`. It exposes a specified Flask port and depends on the `flask_database` service. It includes environment variables for database URI and others.
- **Flask Database**: Built from a Dockerfile in `./postgres/Dockerfile`. It exposes a specified PostgreSQL port and is configured with PostgreSQL environment variables. It includes a volume for data persistence and a health check.
- **PgAdmin**: Based on `dpage/pgadmin4`, it depends on the `flask_database` service, exposes port 5050, and is configured with PGAdmin default email and password.
- **The entire deployment is organized within a custom network named `network`**, and a volume named `postgres_data` is defined for database persistence. This setup implies a sophisticated and integrated environment, primarily focused on Kafka-based data processing and management, with Flask serving as the web framework.

## Backend Data Infrastructure to Front Flask App Connections

Our team has meticulously designed and implemented a robust backend-to-frontend infrastructure within our Flask application. This architecture is strategically developed to ensure an efficient and seamless flow of data from Kafka streams to our PostgreSQL database, and ultimately to the frontend UI. Full set-up instructions are shown in `SETUP_INSTRUCTIONS.md`. The following table provides a detailed summary of the integral components and their specific roles:

| Component                        | Description                                                                                                                                                                                                                                                                                                                                          |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Models Directory**             | Hosts the definitions of our data models, which are directly mapped to database tables via FlaskSQLAlchemy. Key files include location.py, route.py, scheduled_arrival.py, schedule.py, stop.py, trip.py, and vehicle.py. They structure the data received from Kafka and stored in our PostgreSQL database.                                         |
| **views.py**                     | Central to defining route configurations for our Flask application, acting as a gateway for data requests and responses. It facilitates the interaction between the frontend and Flask backend, including routes for data retrieval and handling updates from Kafka. The index route renders the homepage with train schedules and stop information. |
| **extensions.py**                | Contains the FlaskSQLAlchemy extension, crucial for database operations. This extension is used for all database interactions.                                                                                                                                                                                                                       |
| **Dockerfile**                   | Outlines the containerization strategy for our Flask application. It provides insight into the runtime environment, especially highlighting the use of Python 3.10 and the port exposure for the Flask app.                                                                                                                                          |
| **init.py in the App Directory** | Serves as the entry point for creating and configuring the Flask application. It initializes and registers the Flask app, database, and routes, sets the app environment, and ensures database tables are created awt startup.                                                                                                                       |
| **Data Flow Overview**           | Data ingested by our Kafka producer is sent to Kafka topics and then consumed by our service. The consumed data is processed and stored in the PostgreSQL database using the structures defined in our models directory. Data is retrieved from the database through routes in views.py for the frontend, ensuring an interactive user interface.    |

## Conclusion

### Adherence to Industry-Grade Practices

- **Commitment to Excellence**: We have meticulously adhered to industry-grade practices, emphasizing efficiency, robustness, and scalability in our application development.
- **Technological Integration**: Our choices to integrate Kafka, Flask, and XGBoost demonstrate our commitment to delivering a scalable and user-centric solution.
- **Strategic Utilization of Tools**: Leveraging Google Cloud Platform and GitHub Actions for CI/CD underlines our alignment with modern software development practices and the markscheme objectives.

### Forward-Thinking Framework

- **Beyond Immediate Requirements**: Our approach has created a framework that is adaptable to future technological advancements and evolving user needs.
- **Resilience and Scalability**: The modularity of our architecture, combined with cutting-edge tools, ensures scalability and resilience.

### Preparing for Milestone 6

- **Integration of Machine Learning**: We are poised to integrate our machine learning application with our robust backend architecture in Milestone 6.
- **Strategic Use of Vertex AI**: Aligning with the markscheme, we will utilize Google Cloud's Vertex AI for our XGBoost model, embracing cloud-native technologies and enabling auto-scaling to Kubernetes.
- **Industry Best Practices**: This decision exemplifies our commitment to adopting industry best practices, ensuring our deployment is both academically compliant and industry-relevant.
- **Streaming and Predictive Inference**: Achieving accurate inference in streaming pipelines can be challenging, demanding expertise in both streaming and machine learning inference. This expertise is essential to minimize latency and develop applications that offer genuine utility to end-users while avoiding excessive latency or performance degradation to the point of unusability. Apache offers several powerful streaming platforms that excel in the field of ML streaming. Notably, Spark's support for structured streaming and Kafka stand out as excellent options.

### Balancing Academic and Industry Standards

- **Academic and Industry Relevance**: Our project journey balances academic criteria with industry best practices, ensuring relevance and future-proofing.
- **Excitement for Integration**: The upcoming integration of our data infrastructure, frontend, and the Vertex AI model on GCP in Milestone 6 marks a significant step towards building a state-of-the-art application.

In summary, our project is a testament to achieving a harmonious balance between academic requirements and industry standards, positioning us for success in both realms. We eagerly anticipate the integration and advancements in Milestone 6, showcasing our commitment to excellence in application development.


# Milestone 5

## Project Organization

```
.
├── assets
│   ├── architecture
│   │   ├── kafka-producer.svg
│   │   └── kafka-stream.svg
│   └── figures
│       ├── data-streaming.svg
│       ├── high-level.svg
│       ├── mobility_ai_logo.png
│       ├── wanddb_monitoring.pdf
│       └── wanddb_monitoring.png
├── data_streaming
│   ├── kafka_producer
│   │   ├── config.py
│   │   ├── Dockerfile
│   │   ├── __init__.py
│   │   ├── producer.py
│   │   └── README.md
│   ├── kafka_streams
│   │   ├── kafka-connect-jdbc
│   │   │   └── build_connectors.sh
│   │   ├── ksqldb-server
│   │   │   └── queries.sql
│   │   └── README.md
│   └── README.md
├── dev-requirements.txt
├── docker-compose.yml
├── flask_app
│   ├── app
│   │   ├── config
│   │   │   ├── config.py
│   │   │   └── __init__.py
│   │   ├── extensions.py
│   │   ├── __init__.py
│   │   ├── models
│   │   │   ├── __init__.py
│   │   │   ├── location.py
│   │   │   ├── route.py
│   │   │   ├── scheduled_arrival.py
│   │   │   ├── schedule.py
│   │   │   ├── stop.py
│   │   │   ├── trip.py
│   │   │   └── vehicle.py
│   │   ├── static
│   │   │   ├── about.txt
│   │   │   ├── android-chrome-192x192.png
│   │   │   ├── android-chrome-512x512.png
│   │   │   ├── apple-touch-icon.png
│   │   │   ├── css
│   │   │   │   └── styles.css
│   │   │   ├── favicon-16x16.png
│   │   │   ├── favicon-32x32.png
│   │   │   ├── favicon.ico
│   │   │   ├── images
│   │   │   │   ├── mobility_ai_logo.png
│   │   │   │   └── train-icon.png
│   │   │   ├── js
│   │   │   │   ├── index.js
│   │   │   │   └── stops.js
│   │   │   └── site.webmanifest
│   │   ├── templates
│   │   │   ├── about.html
│   │   │   ├── base.html
│   │   │   ├── copyright.html
│   │   │   ├── index.html
│   │   │   ├── navigation.html
│   │   │   └── terms.html
│   │   └── views.py
│   └── Dockerfile
├── LICENSE
├── machine_learning_app
│   ├── assets
│   │   └── vertex_ai.jpg
│   ├── Dockerfile.training
│   ├── mbta_ml
│   │   ├── authenticate.py
│   │   ├── config.py
│   │   ├── data
│   │   │   ├── ml_transit_training_data.csv
│   │   │   └── raw_transit_data.csv
│   │   ├── etl
│   │   │   ├── gcp_dataloader.py
│   │   │   └── __init__.py
│   │   ├── experiments
│   │   │   ├── 25_10_2023
│   │   │   ├── 26_10_2023
│   │   │   └── 27_10_2023
│   │   ├── ml
│   │   │   ├── __init__.py
│   │   │   ├── ml_utils.py
│   │   │   └── xgboost_trainer.py
│   │   ├── models
│   │   ├── production_models
│   │   │   └── final_best_xgboost.json
│   │   └── README.md
│   ├── poetry.lock
│   ├── pyproject.toml
│   ├── README.md
│   └── requirements.txt
├── milestone_submissions
│   ├── README.md
│   └── vertex_ai.jpg
├── postgres
│   ├── db
│   │   └── init.sql
│   └── Dockerfile
├── pytest.ini
├── README.md
├── requirements.txt
├── SETUP_INSTRUCTIONS.md
└── tests
    ├── __init__.py
    └── kafka_producer_configs_test.py

```

## App Design, Setup, and Code Organization

In designing the architecture of our application, we have meticulously embraced a microservices-based approach, a decision rooted in industry best practices for scalable, robust software engineering. This approach is not only about achieving modularity but also about ensuring future scalability, vital for real-world applications. Our code organization is structured with an acute emphasis on maintainability and clarity, aligning with the highest standards of software engineering and the milestone markschemes.

![Screencase of MS5 video app](../assets/ms5_mbta_screen_record.gif)

- **Kafka Producer**: Implementing a Kafka Producer to initiate data ingestion represents a strategic choice for efficient data stream management. This component efficiently channels data streams to Kafka Message Brokers, exemplifying a modern approach to handling large volumes of data in real-time applications.
- **Kafka Message Broker**: The Kafka Message Broker is the backbone of our architecture. It adeptly manages data flow between components, ensuring both robustness and scalability. This choice underpins our commitment to building a system that can handle high throughput and low latency, essential for large-scale, real-time data processing.
- **Data Processing to PostgreSQL**: Our utilization of PostgreSQL for data processing and storage aligns with industry norms for reliable and effective data management. This setup ensures high performance, particularly in handling complex queries and vast datasets.
- **FlaskSQLAlchemy Integration**: The integration of FlaskSQLAlchemy is a testament to our dedication to seamless interaction between our Flask applications and the PostgreSQL database. This ORM (Object-Relational Mapping) tool simplifies database transactions, enhancing the efficiency and reliability of our data operations.
- **Flask with MapBox and Google Client**: Enriching our Flask application with dynamic data visualization tools such as MapBox and Google Client showcases our focus on creating an interactive and user-friendly frontend. These tools elevate the user experience, providing intuitive and informative data visualizations.
- **FastAPI for Model Serving**: Employing FastAPI for model serving aligns with contemporary practices for API development. This framework is known for its high performance and ease of use, making it an ideal choice for exposing our XGBoost model through a RESTful API, thereby ensuring streamlined communication across services.
- **XGBoost Model on Google Vertex AI**: Hosting our XGBoost model on Google Vertex AI aligns with the markscheme and demonstrates the utilization of modern cloud tools. This setup benefits from Kubernetes-powered autoscaling, ensuring that our machine learning model is both efficient and scalable, ready to handle varying loads with ease. It is important to note that our architecture is flexible to prevent vendor lock-in and increasing cloud costs, as we could host our applications with an "on-premise" infrastructure avoiding expensive Google costs. To adhere to the markscheme for milestone 6 we will integrate using Vertex AI but it is important to note we have factored in real-world industrial considerations.
- **API Development**: Our focus on developing robust APIs using FastAPI and Flask is a reflection of our commitment to effective backend-frontend communication. This is crucial for creating a seamless and responsive user experience, vital for modern web applications.
- **Frontend Design**: The intuitive and responsive design of our frontend, integrated seamlessly with backend APIs, ensures real-time data interactions. This approach to frontend design reflects a deep understanding of user needs and industry trends, prioritizing accessibility and responsiveness.
- **User Interface**: The integration of MapBox and Google Client within the user interface offers an interactive and informative user experience. This aspect of our project not only enhances user engagement but also serves as a model for effective data presentation in web applications.

![High Level Architecture](../assets/figures/high-level.svg)

Our architecture's flexibility and scalability make it suitable not just for the MBTA in Boston but also for potential expansion across America. The robustness of our design ensures that our state-of-the-art architecture can be scaled and adapted to various scenarios, making it an exemplary model in the realm of real-time data processing and visualization applications.

## Deployment Strategy and CI/CD

As an alternative to using Ansible as suggested by the markscheme, we plan on using GitHub Actions for our Continuous Integration and Continuous Deployment (CI/CD) pipeline, complemented by deployment on Google Cloud Platform (GCP) (MS6 Requirement). This strategic decision was driven by the desire to align with industry best practices, which increasingly favor the use of integrated CI/CD tools and cloud-native solutions for deployment. GitHub Actions provide a powerful, flexible, and developer-friendly environment for automating software workflows, directly within our code repository ecosystem. Coupled with GCP's scalable and reliable infrastructure, this approach not only meets but exceeds the robustness and efficiency requirements of modern application deployment strategies.

- **GitHub Actions & GitHub for CI/CD**: Leveraging these tools allowed us to automate our software development processes, from testing to deployment, within the GitHub ecosystem.
- **GCP Deployment**: Utilizing the robust and scalable infrastructure of Google Cloud Platform enabled us to achieve a high level of efficiency and reliability in our application deployment.
- **Streamlined Deployment Process**: Our CI/CD pipeline, built around GitHub Actions and GCP, ensures consistent and efficient deployment, enabling swift rollouts of updates and improvements, while maintaining high standards of security and compliance.

Our `docker-compose.yml` contains all the instructions required to deploy our application:

- **Zookeeper Service**: Uses the `confluentinc/cp-zookeeper:7.5.0` image, with the specified port and environment variables. It includes a health check using nc (netcat).
- **Kafka Brokers (broker1, broker2, broker3)**: Each broker uses the `confluentinc/cp-kafka:7.5.0` image. They are configured with individual broker IDs, Zookeeper connection settings, listener settings, and health checks. They depend on the Zookeeper service being healthy.
- **Schema Registry**: Utilizes the `confluentinc/cp-schema-registry:7.5.0` image and depends on the Kafka brokers and Kafka Connect service (`connect`) being healthy. It exposes port 8081 and is configured with environment variables for Kafka and schema registry settings.
- **Kafka Connect**: Based on `confluentinc/cp-kafka-connect:7.5.0`, this service depends on the Kafka brokers and a `flask_database` service. It exposes port 8083 and includes various Kafka Connect configurations, a volume mount for Kafka Connect JDBC, and a health check.
- **Sink Connectors**: Uses the `curlimages/curl:8.4.0` image, depending on `flask_database` and `connect`. It runs a script for building connectors.
- **Control Center**: Utilizes `confluentinc/cp-enterprise-control-center:7.5.0`, depending on Kafka brokers and `ksqldb-server`. It exposes port 9021 and is configured with various environment variables related to Kafka, Connect, KSQL, and schema registry.
- **Producer**: A custom service built from a Dockerfile in `./data_streaming/kafka_producer/Dockerfile`. It depends on Kafka brokers and includes environment variables for Kafka and an API key.
- **KSQLDB Server and CLI**: Both use Confluent's ksqlDB images (`confluentinc/ksqldb-server:0.29.0` and `confluentinc/ksqldb-cli:0.29.0`). The server exposes port 8088 and is configured with various KSQL settings, including a volume for SQL queries. The CLI depends on the server and Kafka brokers.
- **Flask App**: A custom service built from a Dockerfile in `./flask_app/Dockerfile`. It exposes a specified Flask port and depends on the `flask_database` service. It includes environment variables for database URI and others.
- **Flask Database**: Built from a Dockerfile in `./postgres/Dockerfile`. It exposes a specified PostgreSQL port and is configured with PostgreSQL environment variables. It includes a volume for data persistence and a health check.
- **PgAdmin**: Based on `dpage/pgadmin4`, it depends on the `flask_database` service, exposes port 5050, and is configured with PGAdmin default email and password.
- **The entire deployment is organized within a custom network named `network`**, and a volume named `postgres_data` is defined for database persistence. This setup implies a sophisticated and integrated environment, primarily focused on Kafka-based data processing and management, with Flask serving as the web framework.

## Backend Data Infrastructure to Front Flask App Connections

Our team has meticulously designed and implemented a robust backend-to-frontend infrastructure within our Flask application. This architecture is strategically developed to ensure an efficient and seamless flow of data from Kafka streams to our PostgreSQL database, and ultimately to the frontend UI. Full set-up instructions are shown in `SETUP_INSTRUCTIONS.md`. The following table provides a detailed summary of the integral components and their specific roles:

| Component                        | Description                                                                                                                                                                                                                                                                                                                                          |
| -------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Models Directory**             | Hosts the definitions of our data models, which are directly mapped to database tables via FlaskSQLAlchemy. Key files include location.py, route.py, scheduled_arrival.py, schedule.py, stop.py, trip.py, and vehicle.py. They structure the data received from Kafka and stored in our PostgreSQL database.                                         |
| **views.py**                     | Central to defining route configurations for our Flask application, acting as a gateway for data requests and responses. It facilitates the interaction between the frontend and Flask backend, including routes for data retrieval and handling updates from Kafka. The index route renders the homepage with train schedules and stop information. |
| **extensions.py**                | Contains the FlaskSQLAlchemy extension, crucial for database operations. This extension is used for all database interactions.                                                                                                                                                                                                                       |
| **Dockerfile**                   | Outlines the containerization strategy for our Flask application. It provides insight into the runtime environment, especially highlighting the use of Python 3.10 and the port exposure for the Flask app.                                                                                                                                          |
| **init.py in the App Directory** | Serves as the entry point for creating and configuring the Flask application. It initializes and registers the Flask app, database, and routes, sets the app environment, and ensures database tables are created awt startup.                                                                                                                       |
| **Data Flow Overview**           | Data ingested by our Kafka producer is sent to Kafka topics and then consumed by our service. The consumed data is processed and stored in the PostgreSQL database using the structures defined in our models directory. Data is retrieved from the database through routes in views.py for the frontend, ensuring an interactive user interface.    |

## Conclusion

### Adherence to Industry-Grade Practices

- **Commitment to Excellence**: We have meticulously adhered to industry-grade practices, emphasizing efficiency, robustness, and scalability in our application development.
- **Technological Integration**: Our choices to integrate Kafka, Flask, and XGBoost demonstrate our commitment to delivering a scalable and user-centric solution.
- **Strategic Utilization of Tools**: Leveraging Google Cloud Platform and GitHub Actions for CI/CD underlines our alignment with modern software development practices and the markscheme objectives.

### Forward-Thinking Framework

- **Beyond Immediate Requirements**: Our approach has created a framework that is adaptable to future technological advancements and evolving user needs.
- **Resilience and Scalability**: The modularity of our architecture, combined with cutting-edge tools, ensures scalability and resilience.

### Preparing for Milestone 6

- **Integration of Machine Learning**: We are poised to integrate our machine learning application with our robust backend architecture in Milestone 6.
- **Strategic Use of Vertex AI**: Aligning with the markscheme, we will utilize Google Cloud's Vertex AI for our XGBoost model, embracing cloud-native technologies and enabling auto-scaling to Kubernetes.
- **Industry Best Practices**: This decision exemplifies our commitment to adopting industry best practices, ensuring our deployment is both academically compliant and industry-relevant.
- **Streaming and Predictive Inference**: Achieving accurate inference in streaming pipelines can be challenging, demanding expertise in both streaming and machine learning inference. This expertise is essential to minimize latency and develop applications that offer genuine utility to end-users while avoiding excessive latency or performance degradation to the point of unusability. Apache offers several powerful streaming platforms that excel in the field of ML streaming. Notably, Spark's support for structured streaming and Kafka stand out as excellent options.

### Balancing Academic and Industry Standards

- **Academic and Industry Relevance**: Our project journey balances academic criteria with industry best practices, ensuring relevance and future-proofing.
- **Excitement for Integration**: The upcoming integration of our data infrastructure, frontend, and the Vertex AI model on GCP in Milestone 6 marks a significant step towards building a state-of-the-art application.

In summary, our project is a testament to achieving a harmonious balance between academic requirements and industry standards, positioning us for success in both realms. We eagerly anticipate the integration and advancements in Milestone 6, showcasing our commitment to excellence in application development.



# Milestone 6
  - Milestone 6 focused on CI/CD, kubernetes, and spark strucured streaming for the ml predictions.


## Spark Structued Streaming

Kafka serves as the backbone of our entire application, facilitating seamless messaging and data delivery between services. The machine learning component of our application is no exception to this architecture. To seamlessly integrate machine learning predictions into our application, we leveraged Apache Spark's Kafka Stream API and Apache Spark MLLIB.

Our approach began by creating a dedicated consumer group for each Kafka topic we required. This action resulted in the creation of new Kafka streams, effectively enabling real-time data processing within the application. While Apache Spark is commonly associated with batch processing, it possesses the capability to handle streaming data in real time. Spark accomplishes this by queuing mini-batches of data and streaming them one at a time. It's worth noting that this method introduces a slight latency compared to competing streaming frameworks like Apache Flink which uses a method similar to a sliding window. However, for the specific requirements of our application, this level of latency remains acceptable and well within tolerable limits.

### Stream joining

In our Kafka setup, if you recall, the Kafka producer is responsible for streaming data from multiple endpoints over the Server-Sent Events (SSE) protocol, with each endpoint corresponding to a distinct Kafka topic. This distributed data collection is a crucial part of our system. To generate accurate predictions, we face the challenge of consolidating these separate streams into a single cohesive dataframe that we can use for prediction purposes. This process is intricate due to the need to handle late-arriving data.

Late-arriving data refers to a scenario where one stream delivers a feature while another stream is yet to provide the associated feature it depends on. In other words, there's a temporal misalignment in the arrival of data from different streams. This issue becomes particularly pronounced when dealing with geographical location data for trains, as it exhibits rapid and continuous changes compared to other less frequently updated streams.

To address this challenge effectively, spark implements some mechanisms to accommodate late-arriving data gracefully. This specifically involves strategies such as windowing and event-time processing. For this project, we leveraged event time processing and took advantage of KSQLs materialized views. KSQL tracks the key (unique identifier) of each message in the Kafka topic across the partitions. When a key recieves an update, the materialized view will update accordingly. This allows us to simply update our predicions if data arrives late or is updated in the process.


### User defined functions
Spark, to no suprise, doesn't have native functions for predicting arrival times of trains. So we have to implement this behavior ourselfs using custom user defined functions (UDFs). We define a function `_predict_arrival` as the following

```python
import numpy as np
import datetime as datetime

@F.udf(T.TimestampType())
def _predict_arrival(scheduled_time: datetime.datetime, features: List[float]):
    """
    Predict the arrival time for a given vehicle at a stop using XGBoost.

    Parameters
    ----------
    scheduled_time : datetime.datetime
        Scheduled time of train arrival
    features : List[float]
        Input features for the model.

    Returns
    -------
    datetime.datetime
        Predicted arrival time.
    """
    if not features or not all(features):
        return None

    feature_names = ["distance_travel_miles", "sin_time", "cos_time"]
    numpy_features = np.array([features])
    dmatrix = xgb.DMatrix(numpy_features, feature_names=feature_names)

    prediction = model.predict(dmatrix)
    delay = float(prediction[0])

    return scheduled_time + datetime.timedelta(seconds=delay)
```

The UDF above accepts a scheduled arrival time and incorporates a time delta in seconds, either adding or subtracting it, to predict the real-time arrival of the train.

It's essential to acknowledge that while this UDF serves a practical purpose and is relatively straightforward to implement within the context of this project, it does come with certain limitations. One of these limitations is that UDF functions tend to operate at a slower pace because they require the conversion of Python data types to the Java virtual machine for execution within the Spark session's runtime environment. This conversion process introduces latency into the system. Nevertheless, for the specific requirements of the project, this latency is deemed acceptable and falls within the project's constraints.

### Pushing Predictions back into Kafka to be used down stream.

Once the predicion is made by our user defined function we push the prediction back into Kafka to be comsumed by the application. A screenshot of the prediction stream in deployment can be seen below. Each prediciton is pushed as a record back to to kafka to be consumed by downstream.

![High Level Architecture](../assets/figures/predictions-streaming.png)


These predictions are able to be viewed on the front end

![High Level Architecture](../assets/figures/app-demo.gif)


The latency is tolerable, but there is room for improvent in future iterations which I plan on completing in the future.

## CI/CD

Continuous Integration and Continuous Deployment (CI/CD) play a pivotal role in every substantial repository. They enable the seamless integration of updates into the application whenever developers make changes. In our case, our codebase is substantial and follows a monolithic repository approach. Interestingly, this monolithic repository structure proves to be advantageous for our CI/CD pipelines. It allows us to efficiently monitor the impact of changes in one microservice on other services within the same codebase. This visibility is instrumental in detecting and addressing potential issues that might arise when one service undergoes updates, ensuring the overall stability and reliability of
our application.



## Continuious Integration

When new changes are committed to the repository, there is a possibility of introducing issues, underscoring the importance of incorporating unit tests into our development process. In Python, the `unittest` module is readily available as part of the standard library. Nevertheless, crafting assertion statements within unit tests can sometimes be challenging to remember and configuring setup and teardown functions can become intricate. Consequently, we have chosen to leverage pytest due to its user-friendly and streamlined testing capabilities.

## Kubernetes (CI/CD Extension)

Our integration of Kubernetes deployment has seamlessly become an integral part of our  CI/CD pipeline, offering a multitude of benefits that enhance our development workflow. One of the primary advantages is the streamlining of the development cycle, which significantly reduces the manual intervention required in the deployment process.

This efficiency boost is achieved through the inclusion of a YAML configuration file in the .github/workflows directory of our project repository. This file serves as a set of instructions that guide our CI/CD pipeline, orchestrating the entire process of building, testing, and deploying our applications. The utilization of Kubernetes within this pipeline has revolutionized how we handle application deployment.

Here's a more detailed breakdown of the key benefits and components of our Kubernetes-integrated CI/CD pipeline:

1. **Automation:** With Kubernetes, integrated into CI/CD we have automated what was once a labor-intensive and error-prone deployment process. The YAML configuration file defines the desired state of our applications, and Kubernetes takes care of provisioning and scaling resources, ensuring that our applications run consistently across different environments.

2. **Consistency:** By incorporating Kubernetes, we ensure consistency in our deployments. The same YAML file is used across development environments, minimizing the chances of configuration drift or discrepancies between environments.


In summary, our Kubernetes-integrated CI/CD pipeline has revolutionized our development process by automating deployment, ensuring consistency, and providing scalability and robustness. This approach, driven by a YAML configuration file, not only simplifies our workflow but also enhances the reliability, security, and efficiency of our application deployment process.


### Instructions

We've simlpified the instructions into a two step process

## Creating a Kubernetes Manifest with Kompose

To start the deployment process, the first step is to create a manifest artifact. While in a local development setup or within a virtual machine (VM) instance, launching the application is as simple as running the following command: `docker compose up -d`. However, this approach is not suitable for Kubernetes. Kubernetes requires a specific type of configuration called a manifest. To create this manifest, we rely on a Kubernetes tool called [Kompose](https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/).

### Manifest Artifact
In the context of Kubernetes, a manifest is essentially a declaration of how the application should run within the Kubernetes cluster. It specifies various aspects of the application, such as the container images, resource requirements, networking, and scaling rules.


### Transition from Docker Compose to Kubernetes Manifest
In local development or within our GCP VM, `docker compose` serves as a convenient tool for defining and launching multi-container applications. It simplifies the deployment process, making it easy to get the application up and running for development and testing purposes. Kubernetes, on the other hand, operates with a different philosophy. It relies on declarative configurations to manage containerized applications efficiently in a distributed environment. To run an application on Kubernetes, you need to provide a set of YAML or JSON files that describe the desired state of your application.

This is where Kompose comes into play. It serves as a bridge between the `docker compose` world and the Kubernetes ecosystem. Kompose allows us to take the existing Docker Compose configuration, and automatically generates Kubernetes-compatible manifests from it.


### Kubernetes Setup
First, setup kubernetes cluster in GCP.  Users are presented with the option to choose between Autopilot and Standard Kubernetes clusters. It's important to note that our initial development process relied on a docker-compose.yml configuration file with specific port specifications. This choice inadvertently constrained us to using a Standard Kubernetes cluster instead of Autopilot.

Autopilot Kubernetes clusters are designed to simplify cluster management and microservices orchestration by automating many aspects of cluster operation. However, the conflict arises because Autopilot clusters manage certain aspects automatically, including port assignments. As a result, when port specifications are explicitly defined in a docker-compose.yml file, it conflicts with Autopilot's automated management, creating a potential clash in configuration and functionality.

![High Level Architecture](../assets/figures/kubernetes-cluster.png)


### CI/CD Kubernetes Deployment

Once the kubernetes cluster is setup, we can deploy to the cluster automatically using CI/CD and github actions.

First, we configure a YAML file as follows.

```shell
name: Deploy to Kubernetes

on:
  push:
    branches:
      - 'main'

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: "gcp auth"
        uses: "google-github-actions/auth@v1"
        with:
          credentials_json: "${{ secrets.GCP_CREDENTIALS }}"

      - name: Extract environment variables
        run: |
          echo "${{ secrets.ENV_FILE }}" > .env

      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v2'

      - name: Configure gcloud CLI
        run: |
          gcloud config set project ac215-transit-prediction
          gcloud components install gke-gcloud-auth-plugin
          gcloud container clusters get-credentials cluster-1 --region us-central1-c --project ac215-transit-prediction

      - name: Deploy to Kubernetes
        run: kubectl apply -f manifests

```

This file employs service account authentication credentials to authenticate with Google Cloud Platform (GCP). After successful authentication, it proceeds to deploy the previously mentioned manifest to the Kubernetes cluster resource. On pushes to main, the commands get executed and the job runs successfully with the following output


![High Level Architecture](../assets/figures/kubernetes-ci-job.png)
